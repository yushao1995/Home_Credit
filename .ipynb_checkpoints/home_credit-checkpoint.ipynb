{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%% import packages\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-385ee911352e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas_profiling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmissingno\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmsno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import category_encoders as ce\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import MissingIndicator\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from patsy import dmatrices\n",
    "from biokit import corrplot\n",
    "import scipy\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% read data\n"
    }
   },
   "outputs": [],
   "source": [
    "# app = pd.read_csv(\"/Users/dauku/Desktop/Python/homecredit/home-credit-default-risk/application_train.csv\")\n",
    "# Profile = ProfileReport(app, title = \"Pandas profiling Report\", html = {\"style\": {\"full_width\": True}})\n",
    "# Profile.to_file(output_file=\"/Users/dauku/Desktop/Git/DavidKu_IAA2020/Home_Credit/Application_html.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% Cleaning Class\n"
    }
   },
   "outputs": [],
   "source": [
    "class Cleaning:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.data = pd.read_csv(path)\n",
    "        self.summary_report = None\n",
    "        self.vif = None\n",
    "        self.target_freq = None\n",
    "\n",
    "    def get_data(self):\n",
    "        return(self.data)\n",
    "\n",
    "    def summary(self):\n",
    "        uniques = self.data.nunique()\n",
    "        dtypes = self.data.dtypes\n",
    "        missing = self.data.isnull().sum()\n",
    "\n",
    "        report = pd.DataFrame(uniques)\n",
    "        report.columns = [\"uniques\"]\n",
    "        report[\"dtypes\"] = dtypes\n",
    "        report[\"missing\"] = missing\n",
    "        report[\"missing_pct\"] = report.missing / self.data.shape[0]\n",
    "\n",
    "        self.summary_report = report\n",
    "\n",
    "    def get_summary(self):\n",
    "        return(self.summary_report)\n",
    "\n",
    "    def categorical(self):\n",
    "        nunique = self.data.nunique()\n",
    "        binary_list = nunique[nunique == 2].index.tolist()\n",
    "        self.data[binary_list] = self.data[binary_list].astype(\"category\")\n",
    "        # binary_list = self.summary()[self.summary[\"uniques\"] == 2].index.tolist()\n",
    "        # self.data[binary_list] = self.data[binary_list].astype(\"category\")\n",
    "\n",
    "        dtypes = self.data.dtypes\n",
    "        object_list = dtypes[dtypes == \"object\"].index.tolist()\n",
    "        # object_list = self.summary()[self.summary()[\"dtypes\"] == \"object\"].index.tolist()\n",
    "        self.data[object_list] = self.data[object_list].astype(\"category\")\n",
    "\n",
    "    def imputation(self, threshold, drop):\n",
    "        self.summary()\n",
    "        # vars that need imputation\n",
    "        imput_list = self.summary_report[(self.summary_report[\"missing_pct\"] < threshold) & (self.summary_report[\"missing_pct\"] > 0)]\n",
    "        imputing = self.data[imput_list.index]\n",
    "\n",
    "        # vars that don't contain any missings\n",
    "        no_missing_list = self.summary_report[self.summary_report[\"missing_pct\"] == 0]\n",
    "        no_missing = self.data[no_missing_list.index]\n",
    "\n",
    "        # impute categorical variables\n",
    "        imputing_cat = imputing.select_dtypes(exclude=\"number\")\n",
    "        cat_var = imputing_cat.columns\n",
    "        cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")\n",
    "        cat_imputted = pd.DataFrame(cat_imputer.fit_transform(imputing_cat))\n",
    "        cat_imputted.columns = cat_var\n",
    "        cat_imputted = cat_imputted.astype(\"category\")\n",
    "\n",
    "        # imputing numerical variables\n",
    "        imputing_num = imputing.select_dtypes(include=\"number\")\n",
    "        num_var = imputing_num.columns.tolist()\n",
    "        num_var_suffix = [x + \"_indicator\" for x in num_var]\n",
    "        num_var = num_var + num_var_suffix\n",
    "        num_imputer = SimpleImputer(strategy=\"median\", add_indicator=True)\n",
    "        num_imputted = pd.DataFrame(num_imputer.fit_transform(imputing_num))\n",
    "        num_imputted.columns = num_var\n",
    "        num_imputted[num_var_suffix] = num_imputted[num_var_suffix].astype(\"category\")\n",
    "\n",
    "        imputed_data = pd.concat([cat_imputted, num_imputted], axis=1, sort=False)\n",
    "        imputed_data = pd.concat([imputed_data, no_missing], axis=1, sort=False)\n",
    "\n",
    "        # if drop == True:\n",
    "        #     missing_list = self.summary_report[self.summary_report[\"missing_pct\"] != 0]\n",
    "        #     drop_list = imput_list.index.difference(missing_list.index)\n",
    "        self.data = imputed_data\n",
    "        self.summary()\n",
    "\n",
    "    def missing_visualization(self):\n",
    "        sns.heatmap(self.data.isnull(), cbar=False)\n",
    "\n",
    "    def multicollinearity(self):\n",
    "        # Calculating VIF\n",
    "        nums = self.data._get_numeric_data()\n",
    "\n",
    "        vif = pd.DataFrame()\n",
    "        vif[\"factor\"] = [variance_inflation_factor(nums.values, i) for i in range(nums.shape[1])]\n",
    "        vif[\"features\"] = nums.columns\n",
    "        vif_list = vif[vif[\"factor\"] >= 5][\"features\"]\n",
    "        self.vif = vif\n",
    "\n",
    "        nums = nums[vif_list]\n",
    "\n",
    "        # Cluster the correlation matrix\n",
    "        Corr = nums.corr()\n",
    "        d = sch.distance.pdist(Corr.values)\n",
    "        L = sch.linkage(d, method=\"complete\")\n",
    "        ind = sch.fcluster(L, 0.5 * d.max(), \"distance\")\n",
    "        ind = ind.reshape(len(ind), -1)\n",
    "        ind = np.concatenate((ind, np.arange(ind.shape[0]).reshape(ind.shape[0], -1)), axis=1)\n",
    "        ind_sorted = ind[ind[:, 0].argsort()]\n",
    "        columns = [nums.columns.tolist()[i] for i in list(ind_sorted[:, 1])]\n",
    "        ind_sorted = pd.DataFrame(ind_sorted)\n",
    "        ind_sorted.columns = [\"clusters\", \"number\"]\n",
    "        ind_sorted[\"var\"] = columns\n",
    "        freq = ind_sorted[\"clusters\"].value_counts()\n",
    "        ind_sorted = ind_sorted.merge(freq, how=\"left\", left_on=\"clusters\", right_index=True)\n",
    "        ind_sorted_noone = ind_sorted[ind_sorted[\"clusters_y\"] != 1]\n",
    "\n",
    "        # conduct non-parametric ANOVA to decide which variables need to be dropped\n",
    "        cluster_list = np.unique(ind_sorted_noone[\"clusters_x\"].values)\n",
    "        drop_list = []\n",
    "        for i in cluster_list:\n",
    "            vars = ind_sorted_noone[ind_sorted_noone[\"clusters_x\"] == i][\"var\"]\n",
    "            corr = Corr.loc[vars, vars]\n",
    "            corr = corr.where(np.triu(np.ones(corr.shape)).astype(np.bool)).stack().reset_index()\n",
    "            cluster_num = np.ones(corr.shape[0]) * i\n",
    "            cluster_num = cluster_num.reshape(corr.shape[0], -1)\n",
    "            corr = np.concatenate([corr, cluster_num], axis=1)\n",
    "            corr = pd.DataFrame(corr)\n",
    "            corr.columns = [\"row\", \"columns\", \"corr\", \"clusters\"]\n",
    "            corr = corr[corr[\"corr\"] != 1]\n",
    "            if corr.shape[0] == 1:\n",
    "                value = np.array(corr[\"corr\"])\n",
    "                if value < 0.7:\n",
    "                    continue\n",
    "            uniques = np.unique(corr[[\"row\", \"columns\"]].values)\n",
    "            p_value = []\n",
    "            for ii in uniques:\n",
    "                x = self.data[self.data[\"TARGET\"] == 1][ii]\n",
    "                y = self.data[self.data[\"TARGET\"] == 0][ii]\n",
    "                test = stats.kruskal(x, y)\n",
    "                p_value.append(test[1])\n",
    "\n",
    "            min = [i for i, j in enumerate(p_value) if j == max(p_value)]\n",
    "            drop = np.delete(uniques, min)\n",
    "            for var in drop:\n",
    "                drop_list.append(var)\n",
    "\n",
    "        self.data.drop(drop_list, axis = 1, inplace = True)\n",
    "\n",
    "    def vif_corr_map(self):\n",
    "        nums = self.data._get_numeric_data()\n",
    "        vif = pd.DataFrame()\n",
    "        vif[\"factor\"] = [variance_inflation_factor(nums.values, i) for i in range(nums.shape[1])]\n",
    "        vif[\"features\"] = nums.columns\n",
    "        vif_list = vif[vif[\"factor\"] >= 5][\"features\"]\n",
    "        self.vif = vif\n",
    "        nums = nums[vif_list]\n",
    "        Corr = nums.corr()\n",
    "\n",
    "        d = sch.distance.pdist(Corr.values)\n",
    "        L = sch.linkage(d, method=\"complete\")\n",
    "        ind = sch.fcluster(L, 0.5 * d.max(), \"distance\")\n",
    "        ind = ind.reshape(len(ind), -1)\n",
    "        ind = np.concatenate((ind, np.arange(ind.shape[0]).reshape(ind.shape[0], -1)), axis=1)\n",
    "        ind_sorted = ind[ind[:, 0].argsort()]\n",
    "        columns = [nums.columns.tolist()[i] for i in list(ind_sorted[:, 1])]\n",
    "\n",
    "        nums = nums.reindex(columns, axis = 1)\n",
    "        Corr = nums.corr()\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "        cax = ax.matshow(Corr, cmap=\"RdYlBu\")\n",
    "        plt.xticks(range(len(Corr.columns)), Corr.columns, rotation=90)\n",
    "        plt.yticks(range(len(Corr.columns)), Corr.columns)\n",
    "        cbar = fig.colorbar(cax, ticks=[-1, 0, 1], aspect=40, shrink=0.8)\n",
    "\n",
    "    def get_target_freq(self, target):\n",
    "        self.target_freq = self.data[target].value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "app_class = Cleaning(\"/Users/dauku/Desktop/Python/homecredit/home-credit-default-risk/application_train.csv\")\n",
    "# app = app_class.data\n",
    "app_class.summary()\n",
    "summary_report = app_class.get_summary()\n",
    "app_class.categorical()\n",
    "app_class.imputation(0.5, drop = True)\n",
    "app_class.vif_corr_map()\n",
    "app_class.multicollinearity()\n",
    "\n",
    "report = app_class.summary\n",
    "data = app_class.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hi David"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
